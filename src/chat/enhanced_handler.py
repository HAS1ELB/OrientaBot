"""
Chat handler enrichi int√©grant toutes les am√©liorations d'OrientaBot
- Prompts syst√®me enrichis avec personas
- Syst√®me de m√©moire contextuelle 
- Recherche hybride avanc√©e
- D√©tection d'intentions
- Templates de r√©ponses
"""

import json
import re
import streamlit as st
import logging
from groq import Groq
from typing import List, Dict, Optional, Any

# Import des modules de base
from core.config import GROQ_API_KEY, GROQ_MODEL, MAX_TOKENS
from core.contextual_memory import get_contextual_memory_system, get_user_context_for_prompt

# Import des modules d'am√©lioration
from chat.enhanced_prompts import get_enhanced_system_prompt, detect_user_profiles, classify_user_question
from rag.hybrid_search import create_hybrid_search_engine, SearchMode, HybridSearchEngine
from rag.semantic_processor import SemanticDocumentProcessor, convert_to_document_chunk

# Import RAG de base
try:
    from rag.manager import RAGManager
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

logger = logging.getLogger(__name__)

class EnhancedChatHandler:
    """Chat handler avec toutes les am√©liorations int√©gr√©es"""
    
    def __init__(self):
        """Initialise le chat handler enrichi"""
        if not GROQ_API_KEY:
            st.error("‚åö GROQ_API_KEY manquant. Configurez votre .env file avec votre cl√© API Groq.")
            st.stop()
        
        self.client = Groq(api_key=GROQ_API_KEY)
        
        # Syst√®me de m√©moire contextuelle
        self.memory_system = get_contextual_memory_system()
        
        # Processeur s√©mantique pour le chunking avanc√©
        self.semantic_processor = SemanticDocumentProcessor()
        
        # Syst√®me RAG et recherche hybride
        self.rag_manager = None
        self.hybrid_search_engine: Optional[HybridSearchEngine] = None
        self.rag_initialized = False
        
        if RAG_AVAILABLE:
            self._initialize_enhanced_rag()
        
        # Statistiques et m√©triques
        self.response_stats = {
            'total_responses': 0,
            'enhanced_prompts_used': 0,
            'rag_responses': 0,
            'hybrid_search_used': 0,
            'memory_context_used': 0
        }
        
        logger.info("EnhancedChatHandler initialis√© avec toutes les am√©liorations")
    
    def _initialize_enhanced_rag(self):
        """Initialise le syst√®me RAG am√©lior√© avec recherche hybride"""
        try:
            if 'enhanced_rag_manager' not in st.session_state:
                with st.spinner("üöÄ Initialisation du syst√®me RAG avanc√©..."):
                    self.rag_manager = RAGManager()
                    
                    # Initialiser la base de connaissances avec chunking s√©mantique
                    success = self.rag_manager.initialize_knowledge_base()
                    
                    if success:
                        st.session_state.enhanced_rag_manager = self.rag_manager
                        
                        # Cr√©er le moteur de recherche hybride
                        self.hybrid_search_engine = create_hybrid_search_engine(self.rag_manager.vector_store)
                        st.session_state.hybrid_search_engine = self.hybrid_search_engine
                        
                        self.rag_initialized = True
                        
                        # Afficher les statistiques
                        stats = self.rag_manager.get_stats()
                        search_stats = self.hybrid_search_engine.get_search_stats()
                        
                        st.success(f"""
                        üß™ **Syst√®me RAG Avanc√© Activ√©**
                        - **Base vectorielle:** {stats['vector_store_stats']['total_chunks']} chunks s√©mantiques
                        - **Sources:** {stats['pdf_files_count']} documents analys√©s  
                        - **Index mots-cl√©s:** {search_stats['keyword_index_terms']} termes index√©s
                        - **Recherche hybride:** Vectorielle + Mots-cl√©s + Boost contextuel
                        """)
                        
                        logger.info("‚úÖ Syst√®me RAG avanc√© initialis√© avec recherche hybride")
                    else:
                        logger.warning("‚ö†Ô∏è √âchec de l'initialisation RAG - Mode connaissances g√©n√©rales")
                        st.warning("üìÅ Mode connaissances g√©n√©rales activ√©. Ajoutez des PDFs pour activer le RAG avanc√©.")
            else:
                self.rag_manager = st.session_state.enhanced_rag_manager
                self.hybrid_search_engine = st.session_state.get('hybrid_search_engine')
                self.rag_initialized = True
                
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation RAG avanc√©: {e}")
            st.error(f"üö´ Erreur syst√®me RAG: {e}")
            self.rag_manager = None
            self.hybrid_search_engine = None
            self.rag_initialized = False
    
    def process_chat_input(self, prompt: str, base_system_prompt: str, temperature: float):
        """
        Traite l'input utilisateur avec toutes les am√©liorations
        
        Args:
            prompt: Message de l'utilisateur
            base_system_prompt: Prompt syst√®me de base (sera enrichi)
            temperature: Temp√©rature pour la g√©n√©ration
        """
        # Incr√©menter les statistiques
        self.response_stats['total_responses'] += 1
        
        # Charger le profil utilisateur
        user_profile = self.memory_system.load_user_profile()
        
        # D√©marrer ou continuer la session de conversation
        if self.memory_system.current_session is None:
            topic = classify_user_question(prompt)
            self.memory_system.start_conversation_session(topic)
        
        # D√©tecter les profils utilisateur depuis le prompt
        detected_profiles = detect_user_profiles(prompt, st.session_state.messages)
        logger.info(f"Profils d√©tect√©s: {detected_profiles}")
        
        # G√©n√©rer le prompt syst√®me enrichi
        enhanced_system_prompt = get_enhanced_system_prompt(
            prompt, 
            st.session_state.messages, 
            base_system_prompt
        )
        self.response_stats['enhanced_prompts_used'] += 1
        
        # Ajouter le contexte de m√©moire utilisateur
        memory_context = get_user_context_for_prompt()
        if memory_context:
            enhanced_system_prompt += memory_context
            self.response_stats['memory_context_used'] += 1
        
        # Augmenter le prompt avec RAG avanc√© si disponible
        if self.rag_initialized and self.hybrid_search_engine:
            enhanced_system_prompt = self._augment_with_hybrid_search(
                prompt, enhanced_system_prompt
            )
            self.response_stats['hybrid_search_used'] += 1
        
        # Ajouter le message utilisateur √† l'historique
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G√©n√©rer la r√©ponse de l'assistant
        with st.chat_message("assistant"):
            response = self._generate_enhanced_response(
                enhanced_system_prompt, 
                temperature
            )
            
            # Ajouter le turn √† la m√©moire contextuelle
            self.memory_system.add_conversation_turn(
                user_message=prompt,
                assistant_response=response,
                detected_intent=classify_user_question(prompt)
            )
            
            # Afficher les informations sur la recherche si RAG utilis√©
            if self.rag_initialized and self.hybrid_search_engine:
                self._show_enhanced_search_info(prompt)
            
            return response
    
    def _augment_with_hybrid_search(self, user_query: str, base_prompt: str) -> str:
        """Augmente le prompt avec la recherche hybride avanc√©e"""
        try:
            # Effectuer la recherche hybride
            search_results = self.hybrid_search_engine.search(
                user_query, 
                top_k=5, 
                mode=SearchMode.AUTO
            )
            
            if not search_results:
                logger.info("Aucun r√©sultat de recherche hybride")
                return self._add_no_context_notice(base_prompt)
            
            # Construire le contexte enrichi
            context_parts = []
            total_length = 0
            max_context_size = 3000
            
            for result in search_results:
                chunk = result.chunk
                
                # Informations sur la source et le score
                source_info = f"**[{chunk.source} - Page {chunk.page_number}]**"
                
                # Informations sur le scoring hybride
                score_info = f"*Score: V:{result.vector_score:.2f} + K:{result.keyword_score:.2f} = H:{result.hybrid_score:.2f}*"
                
                # Mots-cl√©s match√©s
                keywords_info = ""
                if result.matched_keywords:
                    keywords_info = f" | Mots-cl√©s: {', '.join(result.matched_keywords[:3])}"
                
                # Type de contenu si disponible
                content_info = ""
                if hasattr(chunk, 'metadata') and chunk.metadata.get('content_type'):
                    content_info = f" | Type: {chunk.metadata['content_type']}"
                
                chunk_text = f"""
{source_info}
{score_info}{keywords_info}{content_info}

{chunk.content}
"""
                
                if total_length + len(chunk_text) > max_context_size:
                    break
                
                context_parts.append(chunk_text)
                total_length += len(chunk_text)
            
            if not context_parts:
                return self._add_no_context_notice(base_prompt)
            
            # Construire le prompt augment√©
            enhanced_context = "## CONTEXTE SP√âCIALIS√â - SYST√àME RAG AVANC√â AVEC RECHERCHE HYBRIDE\n"
            enhanced_context += f"*Recherche vectorielle + mots-cl√©s + boost contextuel*\n\n"
            
            for part in context_parts:
                enhanced_context += part
            
            augmented_prompt = f"""{base_prompt}

# MODE RAG AVANC√â ACTIV√â - RECHERCHE HYBRIDE

{enhanced_context}

## INSTRUCTIONS SP√âCIALIS√âES:
- Tu as acc√®s √† des informations OFFICIELLES via un syst√®me RAG avanc√© avec recherche hybride
- Les scores indiquent la pertinence: Vectorielle (s√©mantique) + Mots-cl√©s (factuelle) + Hybride (combin√©)
- Les mots-cl√©s match√©s montrent les termes exacts trouv√©s dans les documents
- PRIORIT√â: Utilise ces informations sp√©cialis√©es avant tes connaissances g√©n√©rales
- Cite toujours les sources en mentionnant le score de pertinence
- Si les informations hybrides ne couvrent pas la question, compl√®te avec tes connaissances g√©n√©rales
- Mentionne le type de recherche utilis√© (vectorielle/mots-cl√©s/hybride) dans ta r√©ponse

## SCORING HYBRIDE:
- **Score Vectoriel**: Similarit√© s√©mantique (0-1)
- **Score Mots-cl√©s**: Correspondance factuelle TF-IDF (0-1)  
- **Score Hybride**: Combinaison pond√©r√©e avec boost contextuel

"""
            
            logger.info(f"‚úÖ Prompt augment√© avec recherche hybride: {len(context_parts)} r√©sultats")
            self.response_stats['rag_responses'] += 1
            
            return augmented_prompt
            
        except Exception as e:
            logger.error(f"Erreur lors de l'augmentation hybride: {e}")
            return self._add_no_context_notice(base_prompt)
    
    def _add_no_context_notice(self, base_prompt: str) -> str:
        """Ajoute une notice quand aucun contexte sp√©cialis√© n'est disponible"""
        return f"""{base_prompt}

# MODE CONNAISSANCES G√âN√âRALES
*Aucune information sp√©cialis√©e trouv√©e dans la base de donn√©es pour cette requ√™te*

Tu r√©ponds avec tes connaissances g√©n√©rales du syst√®me √©ducatif marocain en pr√©cisant que:
- Ces informations sont bas√©es sur tes connaissances g√©n√©rales
- Il est recommand√© de v√©rifier sur les sites officiels des √©tablissements
- Pour des conseils plus pr√©cis, l'utilisateur peut fournir des documents sp√©cifiques

"""
    
    def _generate_enhanced_response(self, system_prompt: str, temperature: float) -> str:
        """G√©n√®re une r√©ponse avec streaming et gestion d'erreurs avanc√©e"""
        
        # Pr√©parer les messages pour l'API
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(st.session_state.messages)
        
        # Afficher le statut de traitement
        response_placeholder = st.empty()
        full_response = ""
        
        # Informations de debug si n√©cessaire
        if st.session_state.get('debug_mode', False):
            with st.expander("üîß Informations de debug", expanded=False):
                st.markdown(f"**Profils d√©tect√©s:** {detect_user_profiles(st.session_state.messages[-1]['content'], st.session_state.messages)}")
                st.markdown(f"**Type de question:** {classify_user_question(st.session_state.messages[-1]['content'])}")
                st.markdown(f"**Longueur du prompt syst√®me:** {len(system_prompt)} caract√®res")
                st.markdown(f"**Recherche hybride:** {'‚úÖ Activ√©e' if self.rag_initialized else '‚ùå Non disponible'}")
        
        try:
            # Streaming de la r√©ponse
            for chunk in self._stream_response(messages, temperature):
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå")
            
            # Finaliser l'affichage
            response_placeholder.markdown(full_response)
            
            # Ajouter la r√©ponse √† l'historique
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
            # Extraire et afficher des recommandations structur√©es si disponibles
            recommendations = self._extract_json_recommendations(full_response)
            if recommendations:
                with st.expander("üìä Recommandations structur√©es"):
                    st.json(recommendations)
            
            return full_response
            
        except Exception as e:
            error_msg = f"Erreur lors de la g√©n√©ration: {str(e)}"
            logger.error(error_msg)
            response_placeholder.error(error_msg)
            return error_msg
    
    def _stream_response(self, messages: List[Dict], temperature: float):
        """Stream la r√©ponse depuis l'API Groq"""
        try:
            response = self.client.chat.completions.create(
                model=GROQ_MODEL,
                messages=messages,
                temperature=temperature,
                max_tokens=MAX_TOKENS,
                stream=True,
            )
            
            for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Erreur API: {str(e)}"
    
    def _extract_json_recommendations(self, text: str) -> Optional[Dict]:
        """Extrait les recommandations JSON si pr√©sentes"""
        pattern = r"```json\s*(\{.*?\})\s*```"
        match = re.search(pattern, text, flags=re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        return None
    
    def _show_enhanced_search_info(self, user_query: str):
        """Affiche les informations d√©taill√©es sur la recherche hybride"""
        if not self.hybrid_search_engine:
            return
            
        try:
            # Obtenir les r√©sultats de recherche pour affichage
            search_results = self.hybrid_search_engine.search(user_query, top_k=3)
            
            if search_results:
                with st.expander("üîç Analyse de recherche hybride", expanded=False):
                    st.markdown("### R√©sultats de la recherche avanc√©e")
                    
                    # Type de recherche utilis√©
                    query_type = self.hybrid_search_engine.detect_query_type(user_query)
                    search_mode = self.hybrid_search_engine.select_search_mode(user_query, query_type)
                    
                    st.markdown(f"""
                    **Type de question d√©tect√©:** {query_type.value}  
                    **Mode de recherche optimal:** {search_mode.value}
                    """)
                    
                    # D√©tails des r√©sultats
                    for i, result in enumerate(search_results, 1):
                        with st.expander(f"R√©sultat {i} - {result.chunk.source} (Score: {result.hybrid_score:.3f})"):
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**Scores de pertinence:**")
                                st.markdown(f"- Vectoriel (s√©mantique): {result.vector_score:.3f}")
                                st.markdown(f"- Mots-cl√©s (factuel): {result.keyword_score:.3f}")
                                st.markdown(f"- **Hybride (final): {result.hybrid_score:.3f}**")
                            
                            with col2:
                                if result.matched_keywords:
                                    st.markdown("**Mots-cl√©s match√©s:**")
                                    st.markdown(f"- {', '.join(result.matched_keywords[:5])}")
                                
                                if result.relevance_factors:
                                    st.markdown("**Facteurs de boost:**")
                                    for factor, value in result.relevance_factors.items():
                                        st.markdown(f"- {factor}: {value:.2f}")
                            
                            # Aper√ßu du contenu
                            content_preview = result.chunk.content[:200] + "..." if len(result.chunk.content) > 200 else result.chunk.content
                            st.markdown(f"**Extrait:** {content_preview}")
            else:
                with st.expander("üìÑ Information de recherche", expanded=False):
                    st.info("üîç Aucun r√©sultat sp√©cialis√© trouv√© - Utilisation des connaissances g√©n√©rales du mod√®le")
                    
        except Exception as e:
            logger.error(f"Erreur lors de l'affichage des informations de recherche: {e}")
    
    def get_enhanced_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques compl√®tes du syst√®me enrichi"""
        base_stats = {
            'handler_type': 'Enhanced',
            'response_stats': self.response_stats,
        }
        
        # Stats de m√©moire contextuelle
        if self.memory_system:
            base_stats['memory_stats'] = self.memory_system.get_memory_stats()
        
        # Stats de recherche hybride
        if self.hybrid_search_engine:
            base_stats['search_stats'] = self.hybrid_search_engine.get_search_stats()
        
        # Stats RAG
        if self.rag_manager:
            base_stats['rag_stats'] = self.rag_manager.get_stats()
        
        return base_stats
    
    def toggle_debug_mode(self):
        """Active/d√©sactive le mode debug"""
        current_mode = st.session_state.get('debug_mode', False)
        st.session_state.debug_mode = not current_mode
        
        mode_str = "activ√©" if st.session_state.debug_mode else "d√©sactiv√©"
        st.success(f"Mode debug {mode_str}")
    
    def export_conversation_data(self) -> Dict[str, Any]:
        """Exporte les donn√©es de conversation pour analyse"""
        if not self.memory_system.current_session:
            return {}
        
        return {
            'session_id': self.memory_system.current_session.session_id,
            'start_time': self.memory_system.current_session.start_time,
            'total_turns': len(self.memory_system.current_session.turns),
            'user_profile': self.memory_system.current_profile.__dict__ if self.memory_system.current_profile else None,
            'stats': self.get_enhanced_stats()
        }