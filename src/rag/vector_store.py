"""
Gestionnaire de base vectorielle pour la recherche sÃ©mantique dans le RAG
"""

import os
import json
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from .pdf_processor import DocumentChunk

logger = logging.getLogger(__name__)

class VectorStore:
    """Gestionnaire de base vectorielle avec FAISS et sentence-transformers"""
    
    def __init__(self, 
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
                 vector_db_path: str = "vector_db"):
        """
        Initialise le store vectoriel
        
        Args:
            embedding_model: ModÃ¨le d'embeddings Ã  utiliser
            vector_db_path: Chemin vers le dossier de la base vectorielle
        """
        self.embedding_model_name = embedding_model
        self.vector_db_path = Path(vector_db_path)
        self.vector_db_path.mkdir(exist_ok=True)
        
        # Fichiers de persistance
        self.index_path = self.vector_db_path / "faiss_index.bin"
        self.chunks_path = self.vector_db_path / "chunks.pkl"
        self.metadata_path = self.vector_db_path / "metadata.json"
        
        # Initialiser le modÃ¨le d'embeddings
        self._load_embedding_model()
        
        # Charger la base existante si elle existe
        self.index = None
        self.chunks = []
        self.metadata = {}
        
        if self._database_exists():
            self.load_database()
    
    def _load_embedding_model(self):
        """Charge le modÃ¨le d'embeddings"""
        try:
            logger.info(f"Chargement du modÃ¨le d'embeddings: {self.embedding_model_name}")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            self.embedding_dimension = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(f"Dimension des embeddings: {self.embedding_dimension}")
        except Exception as e:
            logger.error(f"Erreur lors du chargement du modÃ¨le d'embeddings: {e}")
            raise
    
    def _database_exists(self) -> bool:
        """VÃ©rifie si une base vectorielle existe dÃ©jÃ """
        return (self.index_path.exists() and 
                self.chunks_path.exists() and 
                self.metadata_path.exists())
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        CrÃ©e des embeddings pour une liste de textes
        
        Args:
            texts: Liste des textes Ã  encoder
            
        Returns:
            Array numpy des embeddings
        """
        try:
            embeddings = self.embedding_model.encode(
                texts, 
                show_progress_bar=True,
                normalize_embeddings=True
            )
            return embeddings
        except Exception as e:
            logger.error(f"Erreur lors de la crÃ©ation des embeddings: {e}")
            raise
    
    def build_index(self, chunks: List[DocumentChunk]) -> None:
        """
        Construit l'index vectoriel Ã  partir des chunks
        
        Args:
            chunks: Liste des chunks de documents
        """
        if not chunks:
            logger.warning("Aucun chunk fourni pour construire l'index")
            return
        
        logger.info(f"Construction de l'index vectoriel avec {len(chunks)} chunks...")
        
        # Extraire les textes
        texts = [chunk.content for chunk in chunks]
        
        # CrÃ©er les embeddings
        logger.info("CrÃ©ation des embeddings...")
        embeddings = self.create_embeddings(texts)
        
        # CrÃ©er l'index FAISS
        logger.info("Construction de l'index FAISS...")
        self.index = faiss.IndexFlatIP(self.embedding_dimension)  # Inner Product pour similaritÃ© cosine
        self.index.add(embeddings.astype('float32'))
        
        # Stocker les chunks
        self.chunks = chunks
        
        # CrÃ©er les mÃ©tadonnÃ©es
        from datetime import datetime
        self.metadata = {
            'total_chunks': len(chunks),
            'embedding_model': self.embedding_model_name,
            'embedding_dimension': self.embedding_dimension,
            'sources': list(set(chunk.source for chunk in chunks)),
            'creation_time': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Index vectoriel crÃ©Ã© avec {len(chunks)} chunks")
        logger.info(f"ðŸ“ Sources: {', '.join(self.metadata['sources'])}")
    
    def save_database(self) -> None:
        """Sauvegarde la base vectorielle sur disque"""
        try:
            logger.info("Sauvegarde de la base vectorielle...")
            
            # Sauvegarder l'index FAISS
            if self.index is not None:
                faiss.write_index(self.index, str(self.index_path))
            
            # Sauvegarder les chunks
            with open(self.chunks_path, 'wb') as f:
                pickle.dump(self.chunks, f)
            
            # Sauvegarder les mÃ©tadonnÃ©es
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            logger.info("âœ… Base vectorielle sauvegardÃ©e")
            
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde: {e}")
            raise
    
    def load_database(self) -> None:
        """Charge la base vectorielle depuis le disque"""
        try:
            logger.info("Chargement de la base vectorielle existante...")
            
            # Charger l'index FAISS
            if self.index_path.exists():
                self.index = faiss.read_index(str(self.index_path))
            
            # Charger les chunks
            if self.chunks_path.exists():
                with open(self.chunks_path, 'rb') as f:
                    self.chunks = pickle.load(f)
            
            # Charger les mÃ©tadonnÃ©es
            if self.metadata_path.exists():
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            
            logger.info(f"âœ… Base vectorielle chargÃ©e: {len(self.chunks)} chunks")
            if self.metadata.get('sources'):
                logger.info(f"ðŸ“ Sources: {', '.join(self.metadata['sources'])}")
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement: {e}")
            raise
    
    def search(self, query: str, top_k: int = 5, score_threshold: float = 0.5) -> List[Tuple[DocumentChunk, float]]:
        """
        Recherche sÃ©mantique dans la base vectorielle
        
        Args:
            query: RequÃªte de recherche
            top_k: Nombre de rÃ©sultats Ã  retourner
            score_threshold: Score minimum pour les rÃ©sultats
            
        Returns:
            Liste des chunks trouvÃ©s avec leurs scores
        """
        if self.index is None or not self.chunks:
            logger.warning("Base vectorielle non initialisÃ©e")
            return []
        
        try:
            # CrÃ©er l'embedding de la requÃªte
            query_embedding = self.create_embeddings([query])
            
            # Rechercher dans l'index
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            # Filtrer et formater les rÃ©sultats
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if score >= score_threshold and idx < len(self.chunks):
                    chunk = self.chunks[idx]
                    results.append((chunk, float(score)))
            
            logger.info(f"Recherche pour '{query[:50]}...': {len(results)} rÃ©sultat(s)")
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors de la recherche: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de la base vectorielle"""
        stats = {
            'total_chunks': len(self.chunks),
            'index_size': self.index.ntotal if self.index else 0,
            'embedding_dimension': self.embedding_dimension,
            'sources': self.metadata.get('sources', []),
            'database_exists': self._database_exists()
        }
        
        if self.chunks:
            # Statistiques sur les chunks
            chunk_sizes = [len(chunk.content) for chunk in self.chunks]
            stats.update({
                'avg_chunk_size': np.mean(chunk_sizes),
                'min_chunk_size': np.min(chunk_sizes),
                'max_chunk_size': np.max(chunk_sizes)
            })
        
        return stats
    
    def rebuild_index(self, chunks: List[DocumentChunk]) -> None:
        """Reconstruit complÃ¨tement l'index vectoriel"""
        logger.info("Reconstruction complÃ¨te de l'index vectoriel...")
        
        # Supprimer l'ancienne base
        self.clear_database()
        
        # Construire la nouvelle base
        self.build_index(chunks)
        
        # Sauvegarder
        self.save_database()
        
        logger.info("âœ… Index vectoriel reconstruit")
    
    def clear_database(self) -> None:
        """Supprime la base vectorielle"""
        logger.info("Suppression de la base vectorielle...")
        
        # Supprimer les fichiers
        for file_path in [self.index_path, self.chunks_path, self.metadata_path]:
            if file_path.exists():
                file_path.unlink()
        
        # RÃ©initialiser les variables
        self.index = None
        self.chunks = []
        self.metadata = {}
        
        logger.info("âœ… Base vectorielle supprimÃ©e")