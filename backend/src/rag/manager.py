"""
Gestionnaire RAG principal pour OrientaBot
Coordonne la recherche vectorielle et l'augmentation des prompts
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import re

# Fixed imports - use absolute imports from rag module
from .pdf_processor import PDFProcessor, DocumentChunk
from .vector_store import VectorStore

logger = logging.getLogger(__name__)

class RAGManager:
    """Gestionnaire principal du syst√®me RAG pour OrientaBot"""
    
    def __init__(self, 
                 pdf_folder: str = "data/raw",
                 vector_db_path: str = "data/processed",
                 chunk_size: int = 800,
                 chunk_overlap: int = 150,
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialise le gestionnaire RAG
        
        Args:
            pdf_folder: Dossier contenant les PDFs des √©coles
            vector_db_path: Chemin vers la base vectorielle
            chunk_size: Taille des chunks de texte
            chunk_overlap: Chevauchement entre chunks
            embedding_model: Mod√®le d'embeddings √† utiliser
        """
        
        # Chemins
        self.pdf_folder = Path(pdf_folder)
        self.vector_db_path = Path(vector_db_path)
        
        # Cr√©er les dossiers s'ils n'existent pas
        self.pdf_folder.mkdir(parents=True, exist_ok=True)
        self.vector_db_path.mkdir(parents=True, exist_ok=True)
        
        # Composants
        self.pdf_processor = PDFProcessor(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.vector_store = VectorStore(
            embedding_model=embedding_model, 
            vector_db_path=str(vector_db_path)
        )
        
        # Configuration
        self.max_context_chunks = 5
        self.min_relevance_score = 0.6
        self.context_window_size = 3000  # Taille max du contexte en caract√®res
        
        logger.info("RAGManager initialis√©")
        logger.info(f"üìÇ PDFs: {self.pdf_folder}")
        logger.info(f"üóÑÔ∏è Base vectorielle: {self.vector_db_path}")
    
    def initialize_knowledge_base(self, force_rebuild: bool = False) -> bool:
        """
        Initialise la base de connaissances √† partir des PDFs
        
        Args:
            force_rebuild: Force la reconstruction de la base m√™me si elle existe
            
        Returns:
            True si l'initialisation a r√©ussi
        """
        # V√©rifier si les d√©pendances ML sont disponibles
        if not self.vector_store.ml_available:
            logger.warning("ML dependencies not available. RAG system will be disabled.")
            return False
            
        try:
            # V√©rifier si la base existe d√©j√† 
            if not force_rebuild and self.vector_store._database_exists():
                logger.info("Base vectorielle existante trouv√©e")
                try:
                    self.vector_store.load_database()
                    stats = self.vector_store.get_stats()
                    logger.info(f"üìä Statistiques: {stats['total_chunks']} chunks, {len(stats['sources'])} sources")
                    return True
                except Exception as e:
                    logger.warning(f"Erreur lors du chargement de la base existante: {e}")
                    logger.info("Reconstruction de la base...")
                    force_rebuild = True
            
            # V√©rifier que le dossier PDFs existe et contient des fichiers
            if not self.pdf_folder.exists():
                logger.error(f"Dossier PDFs non trouv√©: {self.pdf_folder}")
                return False
            
            pdf_files = list(self.pdf_folder.glob("*.pdf"))
            if not pdf_files:
                logger.warning(f"Aucun PDF trouv√© dans: {self.pdf_folder}")
                return False
            
            logger.info(f"Initialisation de la base de connaissances avec {len(pdf_files)} PDF(s)")
            
            # Traiter tous les PDFs
            all_chunks = self.pdf_processor.process_all_pdfs(self.pdf_folder)
            
            if not all_chunks:
                logger.error("Aucun chunk cr√©√© depuis les PDFs")
                return False
            
            # Construire l'index vectoriel
            self.vector_store.build_index(all_chunks)
            
            # Sauvegarder la base
            self.vector_store.save_database()
            
            logger.info("‚úÖ Base de connaissances initialis√©e avec succ√®s")
            
            # Afficher les statistiques
            stats = self.vector_store.get_stats()
            logger.info(f"üìä Total: {stats['total_chunks']} chunks cr√©√©s")
            logger.info(f"üìö Sources: {', '.join(stats['sources'])}")
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation de la base: {e}")
            return False
    
    def search_knowledge(self, query: str, top_k: int = None) -> List[Tuple[DocumentChunk, float]]:
        """
        Recherche dans la base de connaissances
        
        Args:
            query: Question ou requ√™te de recherche
            top_k: Nombre max de r√©sultats (par d√©faut: self.max_context_chunks)
            
        Returns:
            Liste des chunks pertinents avec leurs scores
        """
        if not self.vector_store.ml_available:
            logger.warning("Cannot search: ML dependencies not available")
            return []
            
        if top_k is None:
            top_k = self.max_context_chunks
        
        try:
            # Pr√©processer la requ√™te
            processed_query = self._preprocess_query(query)
            
            # Rechercher dans la base vectorielle
            results = self.vector_store.search(
                processed_query,
                top_k=top_k,
                score_threshold=self.min_relevance_score
            )
            
            logger.info(f"Recherche: {len(results)} r√©sultat(s) pertinent(s)")
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors de la recherche: {e}")
            return []
    
    def get_context_for_query(self, query: str) -> Optional[str]:
        """
        R√©cup√®re le contexte pertinent pour une requ√™te donn√©e
        
        Args:
            query: Question de l'utilisateur
            
        Returns:
            Contexte format√© ou None si aucun contexte trouv√©
        """
        # Rechercher les chunks pertinents
        search_results = self.search_knowledge(query)
        
        if not search_results:
            logger.info("Aucun contexte pertinent trouv√©")
            return None
        
        # Construire le contexte
        context_parts = []
        total_length = 0
        
        for chunk, score in search_results:
            # V√©rifier si l'ajout de ce chunk d√©passerait la limite
            chunk_text = f"\n**[{chunk.source} - Page {chunk.page_number}]**\n{chunk.content}\n"
            
            if total_length + len(chunk_text) > self.context_window_size:
                break
            
            context_parts.append({
                'text': chunk_text,
                'score': score,
                'source': chunk.source,
                'page': chunk.page_number
            })
            
            total_length += len(chunk_text)
        
        if not context_parts:
            return None
        
        # Formater le contexte final
        context = "## CONTEXTE SP√âCIALIS√â - √âCOLES SUP√âRIEURES MAROCAINES\n"
        context += f"*Source: Documentation officielle des √©tablissements*\n\n"
        
        for part in context_parts:
            context += part['text']
        
        logger.info(f"Contexte construit: {len(context_parts)} chunk(s), {total_length} caract√®res")
        
        return context
    
    def augment_prompt(self, user_query: str, base_prompt: str) -> str:
        """
        Augmente le prompt avec le contexte pertinent
        
        Args:
            user_query: Question de l'utilisateur
            base_prompt: Prompt syst√®me de base
            
        Returns:
            Prompt augment√© avec contexte ou prompt de base si pas de contexte
        """
        context = self.get_context_for_query(user_query)
        
        if context:
            # Prompt avec contexte RAG
            augmented_prompt = f"""{base_prompt}

# MODE RAG ACTIV√â - INFORMATIONS SP√âCIALIS√âES DISPONIBLES

{context}

## INSTRUCTIONS IMPORTANTES:
- Tu as acc√®s √† des informations OFFICIELLES et D√âTAILL√âES sur les √©coles sup√©rieures marocaines
- Utilise PRIORITAIREMENT ces informations pour r√©pondre aux questions sur les √©tablissements mentionn√©s
- Ces donn√©es sont plus r√©centes et pr√©cises que tes connaissances g√©n√©rales
- Cite toujours les sources quand tu utilises ces informations (ex: "Selon la documentation officielle de l'ENSA...")
- Si les informations du contexte ne couvrent pas enti√®rement la question, combine-les avec tes connaissances g√©n√©rales en pr√©cisant la diff√©rence
- Reste dans ton r√¥le de Dr. Karima Benjelloun mais utilise ces donn√©es sp√©cialis√©es

## PRIORIT√â D'INFORMATION:
1. Contexte sp√©cialis√© ci-dessus (PRIORIT√â HAUTE)
2. Tes connaissances g√©n√©rales du syst√®me √©ducatif marocain (PRIORIT√â NORMALE)
3. Si aucune information disponible, indique-le clairement et sugg√®re de v√©rifier sur les sites officiels

"""
            logger.info("‚úÖ Prompt augment√© avec contexte RAG")
            return augmented_prompt
        else:
            # Prompt de fallback vers connaissances g√©n√©rales
            fallback_prompt = f"""{base_prompt}

# MODE CONNAISSANCES G√âN√âRALES
*Aucune information sp√©cialis√©e trouv√©e dans la base de donn√©es pour cette requ√™te*

Tu r√©ponds avec tes connaissances g√©n√©rales du syst√®me √©ducatif marocain en pr√©cisant que:
- Ces informations sont bas√©es sur tes connaissances g√©n√©rales
- Il est recommand√© de v√©rifier sur les sites officiels des √©tablissements
- Tu peux aider avec des conseils g√©n√©raux d'orientation

"""
            logger.info("üìö Fallback vers connaissances g√©n√©rales LLM")
            return fallback_prompt
    
    def _preprocess_query(self, query: str) -> str:
        """
        Pr√©processe la requ√™te pour am√©liorer la recherche
        
        Args:
            query: Requ√™te brute
            
        Returns:
            Requ√™te pr√©process√©e
        """
        # Normaliser le texte
        query = query.lower().strip()
        
        # Remplacer les abr√©viations courantes
        abbreviations = {
            'ensa': '√©cole nationale des sciences appliqu√©es',
            'emsi': '√©cole marocaine des sciences de l\'ing√©nieur',
            'ensam': '√©cole nationale sup√©rieure d\'arts et m√©tiers',
            'emi': '√©cole mohammadia d\'ing√©nieurs',
            'ensias': '√©cole nationale sup√©rieure d\'informatique et d\'analyse des syst√®mes',
            'encg': '√©cole nationale de commerce et de gestion',
            'fsjes': 'facult√© des sciences juridiques √©conomiques et sociales',
            'fst': 'facult√© des sciences et techniques',
            'est': '√©cole sup√©rieure de technologie',
        }
        
        for abbrev, full_name in abbreviations.items():
            query = re.sub(r'\b' + abbrev + r'\b', full_name, query)
        
        return query
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du syst√®me RAG"""
        return {
            'vector_store_stats': self.vector_store.get_stats(),
            'pdf_folder': str(self.pdf_folder),
            'pdf_files_count': len(list(self.pdf_folder.glob("*.pdf"))) if self.pdf_folder.exists() else 0,
            'ml_available': self.vector_store.ml_available,
            'config': {
                'chunk_size': self.pdf_processor.chunk_size,
                'chunk_overlap': self.pdf_processor.chunk_overlap,
                'max_context_chunks': self.max_context_chunks,
                'min_relevance_score': self.min_relevance_score,
                'context_window_size': self.context_window_size
            }
        }
    
    def rebuild_knowledge_base(self) -> bool:
        """Force la reconstruction compl√®te de la base de connaissances"""
        logger.info("üîÑ Reconstruction de la base de connaissances...")
        return self.initialize_knowledge_base(force_rebuild=True)