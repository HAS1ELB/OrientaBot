"""
Gestionnaire RAG principal pour OrientaBot
Coordonne la recherche vectorielle et l'augmentation des prompts
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import re

# Fixed imports - use absolute imports from rag module
from .pdf_processor import PDFProcessor, DocumentChunk
from .vector_store import VectorStore

logger = logging.getLogger(__name__)

class RAGManager:
    """Gestionnaire principal du systÃ¨me RAG pour OrientaBot"""
    
    def __init__(self, 
                 pdf_folder: str = "data/raw",
                 vector_db_path: str = "data/processed",
                 chunk_size: int = 800,
                 chunk_overlap: int = 150,
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialise le gestionnaire RAG
        
        Args:
            pdf_folder: Dossier contenant les PDFs des Ã©coles
            vector_db_path: Chemin vers la base vectorielle
            chunk_size: Taille des chunks de texte
            chunk_overlap: Chevauchement entre chunks
            embedding_model: ModÃ¨le d'embeddings Ã  utiliser
        """
        
        # Chemins
        self.pdf_folder = Path(pdf_folder)
        self.vector_db_path = Path(vector_db_path)
        
        # CrÃ©er les dossiers s'ils n'existent pas
        self.pdf_folder.mkdir(parents=True, exist_ok=True)
        self.vector_db_path.mkdir(parents=True, exist_ok=True)
        
        # Composants
        self.pdf_processor = PDFProcessor(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.vector_store = VectorStore(
            embedding_model=embedding_model, 
            vector_db_path=str(vector_db_path)
        )
        
        # Configuration
        self.max_context_chunks = 5
        self.min_relevance_score = 0.6
        self.context_window_size = 3000  # Taille max du contexte en caractÃ¨res
        
        logger.info("RAGManager initialisÃ©")
        logger.info(f"ğŸ“‚ PDFs: {self.pdf_folder}")
        logger.info(f"ğŸ—„ï¸ Base vectorielle: {self.vector_db_path}")
    
    def initialize_knowledge_base(self, force_rebuild: bool = False) -> bool:
        """
        Initialise la base de connaissances Ã  partir des PDFs
        
        Args:
            force_rebuild: Force la reconstruction de la base mÃªme si elle existe
            
        Returns:
            True si l'initialisation a rÃ©ussi
        """
        # VÃ©rifier si les dÃ©pendances ML sont disponibles
        if not self.vector_store.ml_available:
            logger.warning("ML dependencies not available. RAG system will be disabled.")
            return False
            
        try:
            # VÃ©rifier si la base existe dÃ©jÃ  
            if not force_rebuild and self.vector_store._database_exists():
                logger.info("Base vectorielle existante trouvÃ©e")
                try:
                    self.vector_store.load_database()
                    stats = self.vector_store.get_stats()
                    logger.info(f"ğŸ“Š Statistiques: {stats['total_chunks']} chunks, {len(stats['sources'])} sources")
                    return True
                except Exception as e:
                    logger.warning(f"Erreur lors du chargement de la base existante: {e}")
                    logger.info("Reconstruction de la base...")
                    force_rebuild = True
            
            # VÃ©rifier que le dossier PDFs existe et contient des fichiers
            if not self.pdf_folder.exists():
                logger.error(f"Dossier PDFs non trouvÃ©: {self.pdf_folder}")
                return False
            
            pdf_files = list(self.pdf_folder.glob("*.pdf"))
            if not pdf_files:
                logger.warning(f"Aucun PDF trouvÃ© dans: {self.pdf_folder}")
                return False
            
            logger.info(f"Initialisation de la base de connaissances avec {len(pdf_files)} PDF(s)")
            
            # Traiter tous les PDFs
            all_chunks = self.pdf_processor.process_all_pdfs(self.pdf_folder)
            
            if not all_chunks:
                logger.error("Aucun chunk crÃ©Ã© depuis les PDFs")
                return False
            
            # Construire l'index vectoriel
            self.vector_store.build_index(all_chunks)
            
            # Sauvegarder la base
            self.vector_store.save_database()
            
            logger.info("âœ… Base de connaissances initialisÃ©e avec succÃ¨s")
            
            # Afficher les statistiques
            stats = self.vector_store.get_stats()
            logger.info(f"ğŸ“Š Total: {stats['total_chunks']} chunks crÃ©Ã©s")
            logger.info(f"ğŸ“š Sources: {', '.join(stats['sources'])}")
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation de la base: {e}")
            return False
    
    def search_knowledge(self, query: str, top_k: int = None) -> List[Tuple[DocumentChunk, float]]:
        """
        Recherche dans la base de connaissances
        
        Args:
            query: Question ou requÃªte de recherche
            top_k: Nombre max de rÃ©sultats (par dÃ©faut: self.max_context_chunks)
            
        Returns:
            Liste des chunks pertinents avec leurs scores
        """
        if not self.vector_store.ml_available:
            logger.warning("Cannot search: ML dependencies not available")
            return []
            
        if top_k is None:
            top_k = self.max_context_chunks
        
        try:
            # PrÃ©processer la requÃªte
            processed_query = self._preprocess_query(query)
            
            # Rechercher dans la base vectorielle
            results = self.vector_store.search(
                processed_query,
                top_k=top_k,
                score_threshold=self.min_relevance_score
            )
            
            logger.info(f"Recherche: {len(results)} rÃ©sultat(s) pertinent(s)")
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur lors de la recherche: {e}")
            return []
    
    def get_context_for_query(self, query: str) -> Optional[str]:
        """
        RÃ©cupÃ¨re le contexte pertinent pour une requÃªte donnÃ©e
        
        Args:
            query: Question de l'utilisateur
            
        Returns:
            Contexte formatÃ© ou None si aucun contexte trouvÃ©
        """
        # Rechercher les chunks pertinents
        search_results = self.search_knowledge(query)
        
        if not search_results:
            logger.info("Aucun contexte pertinent trouvÃ©")
            return None
        
        # Construire le contexte
        context_parts = []
        total_length = 0
        
        for chunk, score in search_results:
            # VÃ©rifier si l'ajout de ce chunk dÃ©passerait la limite
            chunk_text = f"\n**[{chunk.source} - Page {chunk.page_number}]**\n{chunk.content}\n"
            
            if total_length + len(chunk_text) > self.context_window_size:
                break
            
            context_parts.append({
                'text': chunk_text,
                'score': score,
                'source': chunk.source,
                'page': chunk.page_number
            })
            
            total_length += len(chunk_text)
        
        if not context_parts:
            return None
        
        # Formater le contexte final
        context = "## CONTEXTE SPÃ‰CIALISÃ‰ - Ã‰COLES SUPÃ‰RIEURES MAROCAINES\n"
        context += f"*Source: Documentation officielle des Ã©tablissements*\n\n"
        
        for part in context_parts:
            context += part['text']
        
        logger.info(f"Contexte construit: {len(context_parts)} chunk(s), {total_length} caractÃ¨res")
        
        return context
    
    def augment_prompt(self, user_query: str, base_prompt: str) -> str:
        """
        Augmente le prompt avec le contexte pertinent
        
        Args:
            user_query: Question de l'utilisateur
            base_prompt: Prompt systÃ¨me de base
            
        Returns:
            Prompt augmentÃ© avec contexte ou prompt de base si pas de contexte
        """
        context = self.get_context_for_query(user_query)
        
        if context:
            # Prompt avec contexte RAG
            augmented_prompt = f"""{base_prompt}

# MODE RAG ACTIVÃ‰ - INFORMATIONS SPÃ‰CIALISÃ‰ES DISPONIBLES

{context}

## INSTRUCTIONS IMPORTANTES:
- Tu as accÃ¨s Ã  des informations OFFICIELLES et DÃ‰TAILLÃ‰ES sur les Ã©coles supÃ©rieures marocaines
- Utilise PRIORITAIREMENT ces informations pour rÃ©pondre aux questions sur les Ã©tablissements mentionnÃ©s
- Ces donnÃ©es sont plus rÃ©centes et prÃ©cises que tes connaissances gÃ©nÃ©rales
- Cite toujours les sources quand tu utilises ces informations (ex: "Selon la documentation officielle de l'ENSA...")
- Si les informations du contexte ne couvrent pas entiÃ¨rement la question, combine-les avec tes connaissances gÃ©nÃ©rales en prÃ©cisant la diffÃ©rence
- Reste dans ton rÃ´le de Dr. Karima Benjelloun mais utilise ces donnÃ©es spÃ©cialisÃ©es

## PRIORITÃ‰ D'INFORMATION:
1. Contexte spÃ©cialisÃ© ci-dessus (PRIORITÃ‰ HAUTE)
2. Tes connaissances gÃ©nÃ©rales du systÃ¨me Ã©ducatif marocain (PRIORITÃ‰ NORMALE)
3. Si aucune information disponible, indique-le clairement et suggÃ¨re de vÃ©rifier sur les sites officiels

"""
            logger.info("âœ… Prompt augmentÃ© avec contexte RAG")
            return augmented_prompt
        else:
            # Prompt de fallback vers connaissances gÃ©nÃ©rales
            fallback_prompt = f"""{base_prompt}

# MODE CONNAISSANCES GÃ‰NÃ‰RALES
*Aucune information spÃ©cialisÃ©e trouvÃ©e dans la base de donnÃ©es pour cette requÃªte*

Tu rÃ©ponds avec tes connaissances gÃ©nÃ©rales du systÃ¨me Ã©ducatif marocain en prÃ©cisant que:
- Ces informations sont basÃ©es sur tes connaissances gÃ©nÃ©rales
- Il est recommandÃ© de vÃ©rifier sur les sites officiels des Ã©tablissements
- Tu peux aider avec des conseils gÃ©nÃ©raux d'orientation

"""
            logger.info("ğŸ“š Fallback vers connaissances gÃ©nÃ©rales LLM")
            return fallback_prompt
    
    def _preprocess_query(self, query: str) -> str:
        """
        PrÃ©processe la requÃªte pour amÃ©liorer la recherche
        
        Args:
            query: RequÃªte brute
            
        Returns:
            RequÃªte prÃ©processÃ©e
        """
        # Normaliser le texte
        query = query.lower().strip()
        
        # Remplacer les abrÃ©viations courantes
        abbreviations = {
            'ensa': 'Ã©cole nationale des sciences appliquÃ©es',
            'emsi': 'Ã©cole marocaine des sciences de l\'ingÃ©nieur',
            'ensam': 'Ã©cole nationale supÃ©rieure d\'arts et mÃ©tiers',
            'emi': 'Ã©cole mohammadia d\'ingÃ©nieurs',
            'ensias': 'Ã©cole nationale supÃ©rieure d\'informatique et d\'analyse des systÃ¨mes',
            'encg': 'Ã©cole nationale de commerce et de gestion',
            'fsjes': 'facultÃ© des sciences juridiques Ã©conomiques et sociales',
            'fst': 'facultÃ© des sciences et techniques',
            'est': 'Ã©cole supÃ©rieure de technologie',
        }
        
        for abbrev, full_name in abbreviations.items():
            query = re.sub(r'\b' + abbrev + r'\b', full_name, query)
        
        return query
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du systÃ¨me RAG"""
        return {
            'vector_store_stats': self.vector_store.get_stats(),
            'pdf_folder': str(self.pdf_folder),
            'pdf_files_count': len(list(self.pdf_folder.glob("*.pdf"))) if self.pdf_folder.exists() else 0,
            'ml_available': self.vector_store.ml_available,
            'config': {
                'chunk_size': self.pdf_processor.chunk_size,
                'chunk_overlap': self.pdf_processor.chunk_overlap,
                'max_context_chunks': self.max_context_chunks,
                'min_relevance_score': self.min_relevance_score,
                'context_window_size': self.context_window_size
            }
        }
    
    def rebuild_knowledge_base(self) -> bool:
        """Force la reconstruction complÃ¨te de la base de connaissances"""
        logger.info("ğŸ”„ Reconstruction de la base de connaissances...")
        return self.initialize_knowledge_base(force_rebuild=True)